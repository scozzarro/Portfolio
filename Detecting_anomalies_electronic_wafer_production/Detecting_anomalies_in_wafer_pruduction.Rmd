---
title: "Detecting anomalies in silicon wafer manufactoring"
author: "Gabriel Scozzarro"
date: "25/05/2021"
output: hrbrthemes::ipsum_pdf
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,  dev = "cairo_pdf")
```


## 1.0 Introduction

In electronics, a wafer (also called a slice or substrate)[1] is a thin slice of semiconductor, such as a crystalline silicon (c-Si), used for the fabrication of integrated circuits
and, in photovoltaics, to manufacture solar cells. The wafer serves as the substrate for microelectronic devices built in and upon the wafer. It undergoes many microfabrication 
processes, such as doping, ion implantation, etching, thin-film deposition of various materials, and photolithographic patterning. Finally, the individual microcircuits are 
separated by wafer dicing and packaged as an integrated circuit.

Silicon wafer manufacturing facilities are equipped with many sensors which monitor the manufacturing process and the semiconductors that are made down the production line.
Currently, much of the data that is generated by the sensors is only used for troubleshooting when a problem arises. A single manufacturing process has thousands, of different 
parameters from sensors, so efficiently determining the source of a problem in a specific process is very difficult.

It's also necessary to specify that in a such complex manufacturing line a failure rate is physiological and inevitable but if you can predict the failure of a specific production 
lot starting from the sensors readings, it will be possible to ensure a reliable delivery to your clients and better schedule the time to delivery and the entire production line, 

### 1.1 Scope of work & Business task

Detecting anomalies during all those manufacturing processes is crucial. However, current methods of anomaly detection often rely on simple excursion detection methods, and manual
inspection of machine sensor data to determine the cause of a problem. In order to improve semiconductor production line quality, machine learning tools can be developed for more 
thorough and accurate anomaly detection. 

## 2.0 Data

The data set provided is avaible on __[Kaggle](https://www.kaggle.com/arbazkhan971/anomaly-detection)__ it contains all the sensors reading (~1500) during the production of 1 
silicon wafer and a 0-1 logic feature stating the detection of anomalies in that specific wafer.

## 3.0 Tools and process

The analysis was performed using R coding language. A complete list of R packages, a data log and the source code are all avaible on the project github repository at this [**link**](https://github.com/scozzarro/Portfolio/tree/main/Detecting_anomalies_electronic_wafer_production).


```{r Tools and process, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE}

library(tidyverse)
library(ppsr)
library(correlationfunnel)
library(tidymodels)
library(xgboost)
library(vip)
library(caret)
library(DMwR)
library(ConfusionTableR)

#Rmd tools
library(hrbrthemes)
library(Cairo)
library(extrafont)
library(kableExtra)

extrafont::loadfonts()

# Data ----

train_df<- read.csv('Data/Train.csv')

```

## 4.0 Analysis

The data set contains `r sum(is.na(train_df))` NA, with `r nrow(train_df)` rows and `r ncol(train_df)-1` features. Each features is a different sensor reading and unfortunately no 
info was provided about what type of sensor or the unit measure. 

As expected the target variable in the data set is very unbalanced. This makes complete sense, since manufacturers have put in place many sophisticated systems to reduce production
failures. 

```{r Analysis 1, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE, out.width = '95%'}
train_df$Class<- as.factor(train_df$Class)

train_df %>% ggplot(aes(Class, fill = Class)) +
             geom_bar(stat = 'count') +
             theme_ipsum() +
             labs(title = 'Target variable balance', subtitle = '0 equal to no defects, 1 equal to defect detected')

```

For the same reason we expect also that some sensors has always a constant reading. This means that some features in the data set has zero variance or near zero variance. 

```{r Analysis 2, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE, out.width = '50%'}

nzv<- nearZeroVar(train_df, saveMetrics = TRUE, allowParallel = TRUE) #near zero variance check

nzv_df<- data.frame(Features = colnames(train_df[,1:1522]), zeroVar = nzv$zeroVar[1:1522], nzv = nzv$nzv[1:1522])

nzv_df %>% group_by(nzv) %>%
           summarise(Count = n()) %>%
           ggplot(aes(nzv, Count, fill = nzv)) +
           geom_bar(stat = 'identity') +
           geom_text(aes(label = Count), vjust = -0.8) +
           theme_ipsum() +
           labs(title = 'Near zero variance analysis')

nzv_df %>% group_by(zeroVar) %>%
           summarise(Count = n()) %>%
           ggplot(aes(zeroVar, Count, fill = zeroVar)) +
           geom_bar(stat = 'identity') +
           geom_text(aes(label = Count), vjust = -0.8) +
           theme_ipsum() +
           labs(title = 'Zero variance analysis')

nzv_features<- which(nzv$zeroVar == TRUE)
nzv_features_colnms<- colnames(train_df[,nzv_features])
```

As expected some sensors has near zero or zero variance. Near zero variance features can still have predicting power in this scenario, since even small changes in sensor reading can
have a big impact in such complex manufacturing process. 

To discover which sensor has the most relevance in the prediction of anomalies prediction power score, correlation funnel and variable importance score were performed.

```{r Analysis 3, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE, out.width = '50%', results = "hide"}
data_model_ppsr<- train_df %>% visualize_pps(y = 'Class', do_parallel = TRUE, n_cores = 7)

data_model_ppsr_dummy<- data_model_ppsr

data_model_ppsr_dummy$data<- data_model_ppsr_dummy$data[order(data_model_ppsr_dummy$data$pps, decreasing = TRUE),]

data_model_ppsr_dummy$data<- head(data_model_ppsr_dummy$data, 20)

data_model_ppsr_dummy + theme_ipsum() + labs(title = 'Features prediction power score', subtitle = 'Model algorithm: tree, metric: F1')



data_model_binned<- train_df %>% binarize()

cor_fun_plot<- data_model_binned %>% correlate(target = Class__1) %>%
                                     plot_correlation_funnel() +
                                     geom_point(size = 2, color = 'aquamarine3')

cor_fun_plot$data<- head(cor_fun_plot$data, 40)

cor_fun_plot + theme_ipsum()


recipe_spec <- recipe(Class ~ ., data = train_df) %>%
               step_dummy(all_nominal(), -Class)


fit_xgb <- workflow() %>%
           add_model(boost_tree(mode = "classification") %>% set_engine("xgboost")) %>%
           add_recipe(recipe_spec) %>%
           fit(train_df)

vip_plot<- fit_xgb$fit$fit$fit %>% vip()

vip_plot + theme_ipsum() + labs(title = 'Variable Importance Plots', subtitle = 'Importance extracted using XgBoost classification model')

nzv_ppsr_df<-data.frame(features = which(data_model_ppsr$data$x %in% colnames(train_df[,nzv_features])),
                       pps = data_model_ppsr$data$pps[which(data_model_ppsr$data$x %in% colnames(train_df[,nzv_features]))])

```

Several features were acknowledged as very important for the purposes of predicting anomalies in all 3 methods performed. If more info were provided a more depth feature engineering 
could have been implemented.

## 5.0 Prediction model

The desired prediction model needs to estimate the probability of each silicon wafer production lot to have anomalies.
Taking in consideration the natural unbalanced nature of this data set, using it to directly train the model is strongly not recommended to avoid poor performance on the minority 
class which in this case is exactly the one needed to be detected. 

For this reason before train the model a data augmentation technique was applied to the data set partition dedicated to train the model. 
The technique applied was SMOTE which stands for Synthetic Minority Oversampling Technique and it works by selecting examples that are close in the feature space, drawing a 
line between the examples in the feature space and drawing a new sample at a point along that line.

Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is 
chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.
For more information, this technique was described by Nitesh Chawla, et al. in their 2002 paper named for the technique titled __[“SMOTE: Synthetic Minority Over-sampling Technique.”](https://arxiv.org/abs/1106.1813)__.

\pagebreak

After SMOTE was applied the new train data set target class proportion is as follow:
```{r Prediction model 1, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE, out.width = '50%'}

train_df<- train_df[,-nzv_features] #removing near zero o zero variance features

Validation_index<- createDataPartition(train_df$Class, p = 0.3, list = FALSE) #create validation set indx

validation_df<- train_df[Validation_index,]
train_df<- train_df[-Validation_index,]

train_df_smt <-  SMOTE(Class~., train_df, perc.over = 200) #balancing df using SMOTE technique

smt_prop_tbl<- as.data.frame(prop.table(table(train_df_smt$Class)))

smt_prop_tbl %>% ggplot(aes(Var1, Freq, fill = Var1)) +
                geom_bar(stat = 'identity') +
                geom_text(aes(label = percent(Freq)), vjust = -0.5) +
                scale_y_continuous(labels = scales::percent) +
                theme_ipsum() +
                labs(title = 'Train data set target class proportion after SMOTE')
```

For the prediction model, the approach taken was a generalist one, more a starting point rather than a complex and definitive solution. Using the [**H2o.ai**](https://www.h2o.ai/) 
platform in auto machine learning configuration, it was possible to create different models using a variety of machine learning algorithms in the same time.

The evaluation of the models was made using AUC metrics.

```{r Prediction model 2, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE, out.width = '50%', results = "hide"}

library(h2o)

h2o.init(nthreads = -1)

train_h20<- as.h2o(train_df_smt)
validation_h20<- as.h2o(validation_df)

x_h2o<- c(1:1522)
y_h2o<- 1523

###4.5.1 H2o auto machine learning 

h2o_aml <- h2o.automl(x = x_h2o, y = y_h2o,
                      training_frame = train_h20,
                      nfolds = 10,
                      max_models = 10,
                      max_runtime_secs_per_model = 600,
                      stopping_metric = 'AUC',
                      stopping_rounds = 4,
                      seed = 123)

h2o_aml_lb <- h2o_aml@leaderboard


``` 

The training process output several model summarized in the following 'leader-board':

```{r Prediction model 3, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE, out.width = '95%'}
h2o_aml_lb %>% kable('latex', digits = 10, caption = 'Prediction model leaderboard', booktabs = T) %>% kable_styling(full_width = FALSE, font_size = 11, latex_options = c("striped", 'condensed','scale_down'))
```

The most performing model details are as follow:

```{r Prediction model 4, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE, out.width = '95%'}


leader_perf<- h2o.performance(h2o_aml@leader, validation_h20)
leader_perf

plot(leader_perf, type = 'pr')

plot(leader_perf, type = 'roc')

h2o.shutdown(prompt = FALSE)

```

## 6.0 Conclusion

In this work a classification model for the detection of anomalies in silicon wafer manufacturing, with an acceptable performance, was successfully generated showing that a machine 
learning approach for this application is feasible and can radically improve the manufacturing process. The performance, even if it is not extreme, is reasonably high considering 
that the model formulation was unprovided of domain knowledge due to the lack of additional information in the data set. 
